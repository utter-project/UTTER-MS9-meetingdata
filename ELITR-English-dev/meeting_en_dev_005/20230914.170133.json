{"model": "gpt-3.5-turbo-16k", "pre_system": "The following is the transcript of a meeting with multiple participants, where each line has a timestamp (e.g. 11:58:37 AM means 11h58mn37s am), the speaker's name and their utterance.", "post_system": "\n\nAs a professional conversational assistant, you can respond to any questions about the meeting, and you can make inferences from the transcripts.", "transcript": "<another_language/>\n<other_noise/>\n(PERSON14) We still didn't have [PERSON11].\nI'm not sure if he will connect\nI expect him to connect.\nSo anyway let's get started.\nThanks for coming.\nAgain we are trying to record this call to see if we are able to summarize it automatically.\n<parallel_talk/>\nDocument we are talking about -\nSo our probably -\nSo so the point of of these meetings is that everybody very loudly says and enters in the list actually in the in the google sheet, that is more important, the number of items that you have worked on and that you think you deserve some <unintelligible/>.\nSo that's the that's the point of the meetings.\nSo that's list.\nSo please make sure that your point from the last week already have a row and then when they have the row I can end the I can end that.\nSo briefly describe all what you did what do you think deserve some <unintelligible/> because it is usefull for at the all all of that.\nSo it is a practise in selling your homework.\nAnd that that aplies also to [PERSON8], [PERSON2] and [PERSON7] of course.\nAnd everybody else who should also come later.\nSo that's yeah -\nOne thing that I want you to say loud is that I've agreed, I was asked by [PERSON9] to give her presentation on [PROJECT2] on the Monday seminar so as from subtitling we are also expecting to describe of what we are doing regular talk there.\nThe date I think the date is possible the 17th February so in little bit more than two weeks from now.\nAnd the goal of office would be obviously to describe the pipeline say where there problems and to invite more people to join us.\nSo that's why I pick this every day.\nSo that's thats one modification for this.\nThis is the early session that I'm <unintelligible/> of approach we need some domain adaptation again.\nSo I need to work on this which is so so this is a good place for better to use English as English models because I think that this english models could be better adapted.\nAlso maybe [PERSON11] can adapt the just the vocabulary of the [PERSON7] talkage.\nSo the standard as that we are using.\nBut that would need input from people are not here.\nFrom [PERSON6] and maybe [PERSON1].\nSo let's let's <unintelligible/> they the optional.\nSo that is the early session for the adaptation.\nAnd then the other domain adaptation session that will need that will be a harsh one.\nSo there will be then other Monday seminars.\nFor these other Monday seminars we don't have to do anything it's just an opportunity to try it again.\nBut one event will require that.\nAnd that is there is two upcoming events so <other_yawn/> actually event big for them maybe.\nSo the Monday seminar on the 17th happens the week after we have some dry run of a workshop where we are describing various language technologies and that's for the [ORGANIZATION5] congress the part like the site activity.\nWe are not only subtitling but we are also advertising for NLP.\nAnd already wednesday of the 12th of February we are having our first dry run.\nAnd the idea is that that we say there will be also recorded and used for domain adaptation later on.\nAnd the second dry run and then the final workshop.\nSo there will be us non-native speakers very likely presenting in English.\nBecause the workshop will be held in English.\nThat would benefit from the same type of domain adaptation as we will need for the 17th of February.\nSo so there two events which have the same type of adaptation.\nIt's NLP and [PROJECT2] on the 12th and the 17th of February.\nSo two weeks from now and a little bit later.\nAnd then the the important big event that I wanted to talk about is the students firm fair thats something which we have done only last year.\nAnd last year nothing was really working.\nAnd we were only reusing the system from [ORGANIZATION4] and it went for single minute of the two days.\nBut it was an important event for data collection because students are presenting their their companies and we record them and they transcribe it and they compete in how well their work- their voice was recognized.\nAnd it's very noisy environment it's like fair or congress in big hall of many stands and there is little a little side It's not really room, it's just a dedicated area where this competition of the firm presentations is is running.\nSo it's a big challenge for equaliate.\nAnd we are also going to writers, well it will depend what the they will let us or not, last year we were we were showing some of the subtitles and because that was were appearing just once.\nBut because there is four hundred high school students, they are able to take pictures of that and they were then sharing that across networks they were laughing at at other teams.\nSo it is it is an risky activity.\nAnd that's a big for us it's a big opportunity to test that.\nSo that's the 18th and till 20th March and for that we would highly benefit from having some profanity filtering.\nSo so this is an invitation.\nThis is one of the to do or wanted items to add some filter and f- populated what words that we don't want to show at all.\nSo it's it's not always only bad words it could be just words related to bad topics. <laugh/>\nSo that's it is top so it's also the questions what to do of these topics then rather we should removes such words or not.\nBut this is something that we have a months or more for.\nAnd we really should work on this.\nSome some censorship upcoding construct.\nSo as you remember on the the day before yesterday were [PERSON4] giving advering talk one [LOCATION1] word which has nothing in relation that was translated as scrotum and so this is exactly the type of word which is bad for high school students.\nSo - <laugh/>\nSo that's an invitation for for well domain adaptation because we don't know in part no no in part but kind of -\nWe may know the names of the companies at least and maybe some description live up what is their core bussiness or topic of business.\nAnd to that we should adapt because they going to present their companies.\nAnd It will be non-native speakers of English.\nSo this is already like a big challenge.\nAnd then the filtering of the profanity would be would be <unintelligible/> are useful for that.\nSo and I would like to ask everybody to to say what they did and think about what activity they would like to work on.\nSo maybe [PERSON16] you can start.\n(PERSON11) Okay.\nThis week I work on do the collection is business for [OTHER3] and English.\nAnd because the from the [OTHER3] thing of the office I have downloaded 1970 to 1980 pdf files which is extracted which was extracted from the [OTHER2] using texted [OTHER1] so just can work in text format.\nAnd 15000 sentences is crawled automatic using automatic clover for English and [OTHER3].\nFrom these supreme article this.\nSo and we need I need clean all the data because some duplicate sentences and some other garbage values.\nSo member [PERSON6] is I think he was busy exams all times.\n(PERSON14) Who?\n(PERSON11) [PERSON6].\n(PERSON14)  Aha, okay.\n(PERSON11) Sometime that United interrupted because -\nSo this is the picture.\n(PERSON14) OK, so so what would be your collaboration with [PERSON6] then if you if you were available?\n(PERSON11) So we will -\nWe will use it -\n(PERSON14) He would use it as language model and he would extract the terms from that or -?\n(PERSON11) I don't know what he is using the tools what is the aplication so it would be nice when we would be -\n(PERSON14) So I didn't catch the details, the numbers.\nMaybe later if you can correct that.\nBut the first which is so far there were mostly in English or only in English or all the languages, that's a mix of languages?\n(PERSON11) <unintelligible/> English and <unintelligible/> [OTHER3].\n(PERSON14) So only only English and [OTHER3].\n(PERSON11) Little bit. Monolingualy.\n(PERSON14) So for the English or for both of them these are languages anyway so there the adaptation by [PERSON6] make sense it's good to put this into the collection and start organizing the collection.\nOk, great, so thanks.\n(PERSON11) So [PERSON11] updated me yesterday.\nThere was some problem of the -\n(PERSON14) So I think I understood the problem from the comment.\nSo if the idea is that the compress sound goes always to network and is directly decompress before begin sent.\nSo have you added the required command?\n(PERSON11) Yes I could interprate.\nIt's there.\n(PERSON14) So [PERSON11] will test it whenever.\n(PERSON11) So that's one thing, second thing.\nI was looking into that MultiSource model output <unintelligible/> output [PROJECT1].\nSo I did some thing one that.\nBut one specific question I had was regarding the profanity filtery and give after [PERSON4] <unintelligible/>.\nSo let it circumstance we had that the that is not actually <unintelligible/> that is not supposed to be a prophane what coming up.\n(PERSON14) <unintelligible/>\n(PERSON11) Yeah, yeah, so but I believe in the putting up the <unintelligible/> top of list.\nDo you think I mean I don't think that briefly most effective thing because we would blocking on many  <unintelligible/>.\nSo better something I would like to aplain like a like to thinkable.\n(PERSON14) So I have an idea but I don't know if we have the person for that and the idea is exactly responding to the to the fake that.\nThe list will never be complete and it will contain many word which we actually like would like to have an as the output or as the input.\nSo the idea is here the bad word was created by an error in the translation system.\nSo somehow there was lighter the Czech word was <another_language>\"\u0161oup\u00e1tko\"</another_language> which is a slider actually and for some reason in some of the corpora this must have been in the same sentence as as the error came from.\nSo it just made up a follow low frequency pattern which was unreliable.\nSo the idea I have in mind is to train empty systems on corpora which are refind to contain only higher frequency words so like safe word corpora or safe vocabulary corpora.\nSafe is not in the sense that it's not bad word but safe in the sense that it has enough occurancies.\nAnd I would like to -\nSo this is if we do this type of filtering firstly together a very huge corpus then we need to set up threshold like what was the safe we can work work frequent boundary and then we need to create the corpus which is somehow limited to these words.\nAnd that is hard, somewhat, but I think that we could do it monolingualy and use back translation to create the other part.\nAnd maybe put out sentences which are <unintelligible/>. \nThere is two ways either we can brought sentences which contain infrequent word infrequent words or we can replace infrequent words in the sentences with some like placeholder things like forgetting.\nSo then we use the rest of the sentence and non just these kept these unknow words.\nAnd the third option is to use this filtering on in the monolingual setting only and use backtranslation to get the other side.\nAnd we need to evaluate which of these approaches is the best interupt.\nSo all preservation of quality and avoidance of bad words.\nSo this is like a large topic and it's totaly unlagged to refiltering on the fly.\nOne further twist in the on the fly furthering would be like to add words live and we go.\nSo the tool that does this filtering should actually be regularly checking this has appearance some list.\nSo then we can immediately prevent <unintelligible/> respond of them.\nWell in the TOK on Tuesday [PERSON4] didn't say <another_language>\"\u0161oup\u00e1tko\"<another_language/> again so the scrotum did not reappear.\nBut still we would like to have a chance to prevent it from that.\nSo that's a like a bigger experiment and I would probably find someone new for that one semestr stars from the students but it's it is an option.\nOkay.\nSo yeah.\nSo your main homework word compression, the the further experiment in word multisource and just this comment on the limitation profanity filtering.\nOkay.\nThank you.\nSo now maybe [PERSON17].\n(PERSON17) So as you may know I finished evaluation framework and it's ready but maybe for some test maybe I should do some a small changes but I think it's ready.\nWe approved that time base and word base segmentation to find the for example where is the may I want to calculated delay I should estimate that time of each word should be the time expected time for each word.\nSo I used two different segmentation for this, one is timebase and second is wordbase anyway everything is almost ready. \nAnd now maybe I need something like <unintelligible/> complete files to test it to test our there framework.\nI prepare a small things myself manualy and it works now. \nSo this is the team and now I me and [PERSON10] working on the I and [PERSON10] are working on the paper for exceptement and I think that's on.\nSo -\n(PERSON14) So the bad thing -\n[PERSON2] is on the call.\nThe bad thing is that I haven't heard from <unintelligible/> -\nA Okay.\nSo there is four -\nI've received that e-mail yesterday from [PERSON1]\nThe MI Corporas the MI Corporas has time stamps for word.\nSo we should be able to convert that.\nSo the problematic setting.\n[PERSON2]?\nAre you on the call?\n(PERSON2) Yes.\n(PERSON14) So the remember that we have the recorded from MI Corporas where your forced alignment totaly fails.\nAnd we have the idea of breaking in a small chunks.\nSo realize and then [PERSON1] has double checked this corporas actually.\n(PERSON2) Yeah I have already read a mail.\n(PERSON14) Yeah, okay.\nSo I think that it would still be interesting to run your forced alignment of this data and we you can now use the existing time stamps to break the long recording into shorter ones.\nSo that you see whether it like cashes up with these shorter segments.\nAnd that is important to know for some future corpora what will not have the time stamps.\n(PERSON2) Okay.\nI will looking to that.\n(PERSON14) But for for the evaluation we have now something that we can do <unintelligible/> we will actually sometime.\nIt appears that thursday lunch time is the bad time.\nLast time it was also very noisy in this -.\n(PERSON17) Maybe we should do it after lunch or maybe in the morning.\n(PERSON14) Yes.\nIt's I don't know I don't know why it's a case normaly recorders it's much quieters.\n(PERSON17) But the question <unintelligible/> corpus we had the English and [LOCATION1] parts or only-\n(PERSON14) So no, we have only one recordings translated into [LOCATION1] and [OTHER3].\n(PERSON17) Okay, so it's a [OTHER3] but we won't have do we have English is are we have - \n(PERSON14) Yes.\nWe have correct English transcript much large of volume and we have limited file only in [LOCATION1].\n(PERSON17) Okay, and the mode) English it's time stamped.\n(PERSON14) Yes.\n(PERSON17) Okay.\n(PERSON14) This is what something which I didn't realise before because I <unintelligible/> with that like intense enough myself.\n(PERSON17) Yes.\nSo in this case I can convert the second we can use it but we can discuss about it later.\n(PERSON14) Yeah, okay.\nSo to cut down, evaluation too is finished.\nNow, why do not -\n<parallel_talk/>\n(PERSON17) Yes, I have a question about this board files.\nIs there any trajectory to how have together?\n(PERSON14) No we don't have that.\n(PERSON17) Because unfortunately I'm not sure I have access to <unintelligible/>.  \nBecause I have access to some of them because it's a little bit - \nSo maybe if we can have name of files. \nIt could help someone somebody like me.\nBecause I know we have lot of these files now.\n(PERSON11) <unintelligible/> create a list of them and then send <unintelligible/>.\n(PERSON17) Maybe could help because I'm confused about this files I cannot find to use.\n(PERSON14) So that's <unintelligible/>.\nAnd now let's move to those who are present remotly.\nSo maybe [PERSON7] if you can start?\nYou understand at all?\n(PERSON7) Yes, yes.\nI have been listening.\nSo I have mainly concentrated on the adaptation of czech ASR for [PERSON4] talk that took place yesterday.\nSo I've experimented with various technics and especially like domain adaptation of a language model and then acoustic fine tuning on [PERSON4] talk.\nSo I think that I was quite happy with result then during the presentation as like quite a lot of domain specific words were recognized by the model.\nSo that's good sign for me that some how works these technics and -\nYes, right now I'm working on a next version of the czech ASR system that should be trained on even more data.\nSo I'm preparing this.\n(PERSON14) Ok, so remember the thing that I said at the beginning.\nPlease enter your achievements also into the google sheets so that thing once for that.\nWhenever you think it something big and I think this the improvement [PERSON4] talk it's very good one so definitely deserve to be listed.  \nPlease let's make sure you do that.\nDon't forget.\n(PERSON7) Yeah, okay.\n(PERSON14) So now let's move to [PERSON2].\n(PERSON2) So first question regarding the compression.\nWhich kind of compression it is.\nAnd It will be applied to all ASR systems?\n(PERSON14) So that's something that we're just testing and [PERSON11] said that he would evaluated.\n[PERSON11] has found the appropriate command flacs so that the audio is compress to mp3 then shiped as mp3 to network and then decompress before being sent to the ASR.\nAnd the question is what effect does this have on the ASR quality.\nSo if the if there is something we can lost of of  <unintelligible/>  then we don't want to do it but if it's like the same then will write to <unintelligible/> because it simply makes the communication more abased less likely to be to be effected by network long.\nSo [PERSON11] will evaluated soon and he can evaluated with everything and because had already been integrated.\nAnd -\nSo so if your assistance -\nI belive could evaluated yourself as well supportly or if your assistance will be already included and delivered stable output which is the problem that we still have.\nThen [PERSON11] would evaluated for you.\n(PERSON2) Yeah.\nJus-\nBecause I noticed when I read the paper about Librispeech that the recordings were before in mp3 format and then they were actually converted into flac format and then I had to convert it into WAV format so and the and the common voice is also in mp3.\nSo I guess that my systems are actually good with mp3 compression.\nSo that's the first thing had.\nThe second thing is I was working on the sound segmentation.\nIt's there is a problem with ASR system because some words get cut in the middle.\nNow my systemlooks in the window with a greedy decoding and looks for pauses bef- between words and I have window that must be at least four seconds long and maximum is eight seconds found find the the most probable pause between the words and then I cut the windows there.\nSo this makes the decoding a bit better.\nI also trained the transformer converting the phonings into graphians and I tested to the settings.\nBut the results were at least for at least for some [ORGANIZATION6] talks they were really bad because the the corpora on which the transform were trade was trained is for casual speak and and fairy tales and so on.\nSo that's actual translation with the with the transformer was quite bad and the transform-hallucinated some words.\nIt was funny laugh.\nAnd I I sent an e-mail to [PERSON13] and I haven't received any any comments to to this new segment- to this new windowing from him.\nSo -\n(PERSON14) Sorry.\nSo you have already integrated right?\nSo anybody can tests it at this news.\n(PERSON2) No, I had some temporary folder for this hot updates but I need some feedback from [PERSON13] of how this works with his segmenter.\nAnd I will have some more recording from the from yesterday so we can use use this non-native English for some find uniquals our models.\n(PERSON14) Okay.\nSo do you estimate that than your segmenter could be operational in the two weeks from now for the Monday talk by 17th or even for the dry run sesion of the workshop on the 12th and you can talk.\n(PERSON2) I hope so but I haven't heard from [PERSON13] yet.\nSo I don't know wheter it works.\nAnd the next problem is that the segmentation should be done on speaker separately because when there is some conversation the windows can overlap to speakers and the problem is then then the transformer translate these sentences into nonsense.\n(PERSON14) The problem is that we don't have speaker diarization on fly so we simply do not know when there is a speaker change.\n(PERSON7) Yeah that's a problem but if if if the talks are only with one speaker then there is no problem with this.\n(PERSON14) So the talks in general are one speaker only but do it also as an for example for the remote calls there conference calls on the interview platform, there each speaker has different chanel so the diarization is there for free and it's not not mixed.\n(PERSON2) That sould be better and the next problem is that the windows overlaped sentences and this causes some problems too.\nAnd I I now I don't have some- now no solution to this problem.\n(PERSON14) Yeah.\nSo so are you waiting for [PERSON13] also please getting dash with [PERSON11] because I think that [PERSON11] made be more responsive these days. \n[PERSON11] is coming back to back to [LOCATION5] on the weekend.\nWho won't be available during the <unintelligible/>\nThen may have more time for for that.\n(PERSON2) Good.\nI will have new better models by [PERSON1] so maybe this will work.\n(PERSON14) Yeah, okay.\nThank you.\nSo everybody please review what have in document I was trying to do something but I'm not perfect I can roll it follow.\nSo the Last person we have here today is [PERSON8].\nSo I don't know if the rest of team has ever met you in person.\nMaybe not.\n[PERSON8] has already worked on the project last june when he was helping us to put all the byplan together and -\nSo I just remember that his reimplementation or his treak of the TEE programme TEE rescue the project because TEE was cashing its output.\nAnd we need the byplans to be wilow cashing so we created an modify version and there was there was one of the early sessions. \nAnd [PERSON8] is now in [LOCATION3].\nSo he can participate only remotedly and there for also whatever he does has to be like self contain as little interaction with others as as possible.\nSo now [PERSON8] please let us know what if you have work on anything.\nI think you work on paraphrasing.\nSo you are in touch with some other colleagues from here who are actually <unintelligible/> today.\nBut in the paraphrasing goal it's not for [PROJECT2] that's for different that's for different project.\nSo is there anything that you have work on it's useful for [PROJECT2] from the subtitling.\n(PERSON5) Yes, so so far I have only worked on the paraphrasing and actually I just w- I just wanted to say that right now I'm just waiting for a virtual machine for the paraphrasing server but otherwise it is done and working.\nOtherwise I was planning to work on to work on the division <unintelligible/> of of the levels of of microphone. \nI know is that still -\n(PERSON14) It is still deleted.\n(PERSON12) Okay, if it is still deleted then I can then I can started next week.\nBut my first priority was the paraphrasi- was the paraphrasing so I didn't didn't <unintelligible/> yet.\n(PERSON14) So that the problem is that [PERSON13] is not responding.\nI remember that I have asked him to check if the virtual machine set up is is reasonable and something like that and then forward to the IT department.\nIf you don't get any response from [PERSON13] even after the weekend then please make sure to to like treasure ask for that yourself without waiting.\n(PERSON5) Okay, okay so I don't have anything interesting to report you right now.\n(PERSON14) Yeah.\nThe visualisation of sound it's something that we will need.\nThis is something that [PERSON11] would check.\nI think it should be simply and totaly independent process for now which anybody could run to adition to be sound acqusition pipeline and would that see which of the input chanell is receiving what output.\nSo it's very similar to Audacity recording chanells except it is not recording.\n(PERSON5) Yeah, okay.\nOu, there is one thing that isn't exactly I don't know important maybe.\nBut then I just saw it today.\nAnd I told you might be interested in it. \nI saw that there's any paper from [ORGANIZATION3] from december on ASR and it is called FastSpeech.\nHave you have you seen it?\n(PERSON14) I heard about it somewhere where have I saw it -\n(PERSON5) Yeah, so I just try it today and it's just basicly sais that it's like three hundred times faster for ASR so so so it could I just told it could actually be used to do ASR like on on the spot and it couldn't have to be sent and the delay problem could be eliminated.\n(PERSON14) ehmm\n(PERSON5) And they also say it is that it is robust and dust nearly eliminates words keeping <unintelligible/> which is also interesting interesting so I can just send it to you if you like.\n(PERSON14) Well the question is wheter it is available as code, it is? Is it?\n(PERSON5) I haven't I haven't get didn't get try yet. <laugh/>\n(PERSON14) So that in that case that would be obviously interesting so that we would try in our set up.\n(PERSON5) Yeah, I <unintelligible/>.\n(PERSON14) Fastspeech <parallel_talk/>, yeah, it seems like it it should <unintelligible/>.\nSo so again.\nI think I wanted <unintelligible/>.\n<parallel_talk/>\nCheck if there is code for that and integrate - <parallel_talk/>\nSo that's that's it.\nThank you.\n(PERSON5) Anyway I do my exams period is ending maybe I would be able to come to [LOCATION5] for for maybe one week or something like that.\n(PERSON14) Yeah, okay.\nAnd a colleague of mine ask me wheter you are from [LOCATION2] or not and I don't remember.\n(PERSON5) No no.\n(PERSON14) Are you? Are you from Kos-\n(PERSON5) No, I'm not, I'm from [LOCATION4].\n(PERSON14) [LOCATION4] okay.\nYeah, so that's different. <laugh/>\nSo, I think that's we should stop again today it's then spent more time we wanted.\nPlease review this list and please think what you to do from the wanted topics.\nSo that next week we should be meeting at the same time, still thursday and -\nOkay, so what so you prefer friday instead of thursday right?\nSo should we all then make do a call for a doodle because thing changed for others as well.\n<parallel_talk/>\nOkay so.\n(PERSON7) Bye.\nThank you much.\n(PERSON5) Bye.\n(PERSON2) Bye.\n", "utterances": [["20230914:170858", "user", " Who attended the meeting ?", 0], ["20230914:170901", "assistant", "The attendees of the meeting were:\n- PERSON14\n- PERSON11\n- PERSON8\n- PERSON2\n- PERSON7\n- PERSON16\n- PERSON17\n- [PERSON5]\nThere was also a mention of [PERSON6], [PERSON1], [PERSON9], and [PERSON13], but it's unclear whether they were present at the meeting or not.", -1], ["20230914:171057", "user", " What was the purpose of the meeting ?", 0], ["20230914:171102", "assistant", "The purpose of the meeting was to discuss and share updates on the progress of various tasks related to the project. The participants were expected to report on the work they had done, discuss any challenges or issues they encountered, and plan for future tasks and collaborations. Additionally, there were specific topics of discussion, such as recording a presentation on [PROJECT2], domain adaptation, profanity filtering, sound segmentation, and paraphrasing. The meeting also aimed to record the call to test if it could be automatically summarized.", 0], ["20230914:171555", "user", " When will [PERSON9] give a presentation ?", 0], ["20230914:171559", "assistant", "The exact date of [PERSON9]'s presentation was not mentioned in the meeting transcript. However, it was mentioned that [PERSON9] was asked to give a presentation on [PROJECT2] at the Monday seminar, and the date mentioned was around February 17th, which is in a little over two weeks from the meeting.", 1], ["20230914:172018", "user", " What is the main event targeted to test the demo system ?", 0], ["20230914:172023", "assistant", "The main event targeted to test the demo system is the Students Firm Fair, which will take place from March 18th to 20th. This event was mentioned as an important opportunity to showcase and evaluate the system's performance in a noisy environment. The fair involves students presenting their companies, and the system will be used to transcribe their presentations. Additionally, there was a mention of the need for profanity filtering during this event.", 1], ["20230914:172153", "user", " What is challenging about this event ?", 0], ["20230914:172203", "assistant", "The Students Firm Fair presents several challenges for the demo system. These challenges include:\n\n1. Noisy Environment: The fair takes place in a large hall with many stands and activities, which can create a challenging acoustic environment. Background noise and crowd conversations can impact the accuracy of the speech recognition system.\n\n2. Real-time Transcription: The system is expected to provide real-time transcription of the student presentations. This requires efficient processing and low latency to keep up with the speech input.\n\n3. Non-native English Speakers: Many of the student presenters may be non-native English speakers. This can introduce additional challenges in accurately transcribing their speech, as accents and language variations may differ from what the ASR system is trained on.\n\n4. Profanity Filtering: Given the nature of the event, where high school students are presenting their companies, it is crucial to filter out any profanity or inappropriate language from the transcriptions. Developing an effective profanity filtering system is important to ensure a suitable and safe experience for the participants.\n\nOverall, the Students Firm Fair provides a real-world testing ground for the demo system, allowing it to be evaluated in a challenging, high-noise environment with non-native English speakers, while also highlighting the need for profanity filtering.", 1], ["20230914:172319", "user", " How many participants are expected to the students firm fair ?", 0], ["20230914:172322", "assistant", "The number of participants expected at the Students Firm Fair was not mentioned in the meeting transcript. However, it was mentioned that there were around 400 high school students who would be presenting their companies at the fair.", 0], ["20230914:172540", "user", " Why profanity filtering is particularly needed for this event ?", 0], ["20230914:172549", "assistant", "Profanity filtering is particularly needed for the Students Firm Fair due to the nature of the event and the audience involved. The fair involves high school students presenting their companies, which means that the participants and attendees are likely to be younger individuals. Given the age group, it is important to ensure a suitable and safe environment for the students.\n\nProfanity filtering helps to prevent the display of inappropriate or offensive language in the transcriptions or subtitles generated by the system. By implementing a profanity filter, the system can avoid displaying any profane or offensive words, ensuring that the content remains appropriate for the young audience and maintains a professional and respectful environment at the fair.\n\nAdditionally, as mentioned in the meeting, there were previous instances where the subtitles were displayed and shared on social networks, resulting in laughter and potential embarrassment for the participants. Implementing profanity filtering helps to mitigate such risks and maintain a positive and supportive atmosphere during the fair.", 0], ["20230914:172955", "user", " Did the system already output some toxic content in the past ?", 0], ["20230914:172959", "assistant", "The meeting transcript does not explicitly mention whether the system had previously output toxic content. However, there was a reference to an incident where an incorrect translation of a word led to a profane word being displayed in the subtitles during a previous talk by [PERSON4]. This incident highlighted the need for profanity filtering and the importance of preventing any inappropriate or offensive language from being generated by the system, especially in events involving young participants like the Students Firm Fair.", 0], ["20230914:173240", "user", " Which profane word was uttered ?", 0], ["20230914:173246", "assistant", "In the meeting transcript, it was mentioned that during a previous talk by [PERSON4], there was an incident where the word \"\u0161oup\u00e1tko\" (which means \"slider\" in Czech) was mistranslated and displayed as a profane word in the subtitles. The specific profane word that resulted from the translation error was not mentioned in the transcript. However, it was referred to as an example of a word that should not be displayed in the subtitles during the Students Firm Fair, as it was inappropriate for the young audience attending the event.", 0], ["20230914:173314", "user", " Which profane English word was uttered ?", 0], ["20230914:173317", "assistant", "The specific profane English word that was uttered during the meeting was not mentioned in the transcript. The focus of the discussion was more on the incident where the Czech word \"\u0161oup\u00e1tko\" was mistranslated and resulted in a profane word being displayed in the subtitles. The exact profane English word that resulted from the translation error was not specified in the transcript.", 0], ["20230914:173324", "user", " Which profane English word was displayed ?", 0], ["20230914:173328", "assistant", "Apologies for the confusion, but the specific profane English word that was displayed in the subtitles during the previous talk by [PERSON4] was not mentioned in the meeting transcript. The transcript refers to the word as an example of an incorrect translation that led to a profane word being shown. The exact profane English word resulting from the translation error was not specified.", -1], ["20230914:173540", "user", " Who was giving a presentation when the subtitling system broke bad ?", 0], ["20230914:173544", "assistant", "In the meeting transcript, it was mentioned that the subtitling system broke and displayed a profane word during a talk given by [PERSON4]. [PERSON4] is the individual who was giving the presentation when the subtitling system encountered the translation error and displayed the inappropriate word in the subtitles.", 1], ["20230914:174845", "user", " What kind of segmentation (PERSON17) is talking about ?", 0], ["20230914:174853", "assistant", "In the meeting transcript, [PERSON17] mentions two types of segmentation: time-based segmentation and word-based segmentation. These segmentation methods are used to determine the boundaries for calculating delay and estimate the expected time for each word in the context of the evaluation framework. \n\nTime-based segmentation involves dividing the audio into segments based on time intervals. For example, a window of four to eight seconds may be used, and the most probable pause between words is identified within that window.\n\nWord-based segmentation, on the other hand, involves segmenting the audio based on the pauses between words. The goal is to identify and separate individual words in order to analyze and measure the delay between them.\n\nBoth types of segmentation help in assessing and evaluating the performance of the system in terms of timing and delays.", -1], ["20230914:175217", "user", " Who is attending remotely ?", 0], ["20230914:175221", "assistant", "In the meeting transcript, it is mentioned that [PERSON8] is attending remotely. Additionally, there is reference to [PERSON5] attending remotely as well. However, it is unclear whether there are any other participants attending remotely, as they are not specifically mentioned in the transcript.", -1], ["20230914:180436", "user", " Who is the (PERSON2) is waiting for to move forward with her segmentation work?", 0], ["20230914:180440", "assistant", "In the meeting transcript, it is mentioned that [PERSON2] is waiting for [PERSON13] to move forward with her segmentation work. [PERSON2] mentions that they need feedback from [PERSON13] on how the segmentation works with [PERSON13]'s segmenter. It is implied that [PERSON13] has knowledge or expertise in the segmentation process, and [PERSON2] is relying on their input before proceeding further.", 1], ["20230914:180715", "user", " Why is speaker diarization not absolutely needed for some talks ?", 0], ["20230914:180723", "assistant", "Speaker diarization, the process of determining who is speaking when in an audio recording, is not absolutely needed for some talks when they involve only one speaker. In the meeting transcript, it was mentioned that if the talks at the event or in a specific context involve only one speaker, then speaker diarization is not necessary.\n\nSpeaker diarization becomes more crucial when multiple speakers are involved, as it helps separate and identify each speaker's segments within the audio recording. However, in cases where there is a single speaker throughout the talk, such as a monologue or a presentation by a single individual, speaker diarization is not essential as there are no overlapping speech segments to distinguish.\n\nTherefore, for talks with a single speaker, the need for speaker diarization is reduced or eliminated, and the focus can be on other aspects of the transcription or analysis process.", 1], ["20230914:180856", "user", " Where was [PERSON11] ?", 0], ["20230914:180900", "assistant", "In the meeting transcript, it is not explicitly mentioned where [PERSON11] was during the meeting. However, [PERSON11] is mentioned as one of the participants who was expected to attend the meeting. It is possible that [PERSON11] may have been present remotely or had connectivity issues preventing them from joining the call.", 0], ["20230914:181041", "user", " Who is a new comer ? ", 0], ["20230914:181044", "assistant", "In the meeting transcript, there is a mention of a new comer who is [PERSON8]. [PERSON8] had previously worked on the project last June and has now joined remotely from [LOCATION3].", 1], ["20230914:181441", "user", " Which person was mentioned several times as needed but not responsive?", 0], ["20230914:181446", "assistant", "In the meeting transcript, [PERSON13] is mentioned several times as someone who was needed for certain tasks but was not responsive. There were mentions of [PERSON13] not responding regarding the setup of a virtual machine for the paraphrasing server and not providing feedback on how the segmentation worked with their segmenter. [PERSON13]'s lack of responsiveness caused some delays and hindered progress in those specific areas.", 1], ["20230914:181710", "user", " What system mentioned could drastically speed up automatic transcription ?", 0], ["20230914:181716", "assistant", "In the meeting transcript, it is mentioned that a system called \"FastSpeech\" was brought up. The system is described as being three hundred times faster for automatic speech recognition (ASR) and has the potential to eliminate the need for sending audio and experiencing delays. FastSpeech is said to be robust and greatly reduces word error rates. However, it is unclear whether the system is already available as code and integrated into the project. Further discussion and investigation may be needed to determine the feasibility of implementing FastSpeech for speeding up automatic transcription.", 1], ["20230914:181927", "user", " Was next meeting date set ?", 0], ["20230914:181929", "assistant", "In the meeting transcript, there is a mention of potentially changing the day of the next meeting from Thursday to Friday. However, it was also suggested to conduct a poll using Doodle to determine the most suitable day for the next meeting. Therefore, it is unclear from the transcript whether a specific date was set for the next meeting.", 1]]}