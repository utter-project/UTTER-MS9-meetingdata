### File description

You will need to unzip (password protected - use 'utter' as a password) the files data.zip and generated-responses.zip first, to retrieve the structure (we provide .zip files instead of raw json and text files in order to avoid data contamination of LLMs trained on web scrapped data) - type unzip data.zip and unzip generated-responses.zip 

After these two unzip commands, this directory contains two folders: `data` and `generated-responses`. The former contains the ELITR-Bench data files with the crafted questions, ground-truth answers and metadata. The latter contains the responses generated on ELITR-Bench with their evaluation. All files are provided in JSON format.

The JSON files are named based on the following template:

- `elitr-bench-{dataset_version}_{split}.json` for the data files;
- `elitr-bench-{dataset_version}_{split}_{inference_mode}_{evaluator}.json` for the generated responses;

where:

- `{dataset_version}` is either the `qa` or `conv` version of ELITR-Bench;
- `{inference_mode}` is either the `st` (single-turn) or `mt` (multi-turn) mode used for inference, i.e., either questions are asked independently or in sequence within the same conversation;
- `{split}` is either the `dev` or `test2` split set;
- `{evaluator}` is either `gpt-4-eval` (indicating that the reported scores are obtained from the GPT-4 evaluator) or `all-eval` (indicating that we report the scores for the 4 different evaluators considered in the "LLM-based evaluation assessment" section of the paper).

### JSON file structure 

Each JSON file is structured as follows:

	{
	  "split": "...",
	  "meetings": [
	    {
	      "id": "...",
	      "questions": [
	        {
	          "id": "...",
	          "question-type": "...",
	          "answer-position": "...",
	          "question": "...",
	          "groundtruth-answer": "...",
	          "generated-responses": [
	            {
	              "model": "...",
	              "generated-response": "...",
	              "gpt-4-eval_score": "..."
	              "prometheus-eval_score": "...",
	              "gold-human-eval_score": "...",
	              "silver-human-eval_score": "..."
	            },
	            ...
	          ]
	        },
	        ...
	      ]
	    },
	    ...
	  ]
	}
	
where:

- `split` is either `dev` or `test2`;
- `meetings` is the list of meetings associated to the set;
- `id` (meeting-level) is the name of the corresponding meeting transcript file from the original ELITR dataset;
- `questions` is the list of questions associated to the meeting;
- `id` (question-level) is a numeric identifier for each question in the meeting;
- `question-type` indicates the type of the questions, among `who`, `what`, `howmany` and `when`;
- `answer-position` indicates the position of the answer in the transcript, among `B` (beginning), `M` (middle), `E` (end) and `S` (several passages across the transcript);
- `question` contains the actual question;
- `groundtruth-answer` contains the manually annotated ground-truth answer to the question;
- `generated-responses` is the list of responses generated by different LLMs to answer the question (this list is only present for the JSON files in the `generated-responses` folder);
- `model` indicates the LLM used to generate the answer;
- `generated-response` contains the actual generated answer;
- `gpt-4-eval_score`, `prometheus-eval_score`, `gold-human-eval_score`, `silver-human-eval_score` indicate the numeric score between 1 and 10 obtained by different evaluators for the generated answer (`prometheus-eval_score`, `gold-human-eval_score`, `silver-human-eval_score` are only reported for the `all-eval` JSON file).
